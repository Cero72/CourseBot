{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-06T22:09:51.528688Z",
     "iopub.status.busy": "2024-12-06T22:09:51.528426Z",
     "iopub.status.idle": "2024-12-06T22:10:04.632794Z",
     "shell.execute_reply": "2024-12-06T22:10:04.631901Z",
     "shell.execute_reply.started": "2024-12-06T22:09:51.528661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.12.2-py3-none-any.whl (365 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2, bitsandbytes, trl, peft\n",
      "Successfully installed PyPDF2-3.0.1 bitsandbytes-0.45.0 peft-0.14.0 trl-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets accelerate bitsandbytes torch peft trl PyPDF2 scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:10:04.634818Z",
     "iopub.status.busy": "2024-12-06T22:10:04.634534Z",
     "iopub.status.idle": "2024-12-06T22:10:06.177896Z",
     "shell.execute_reply": "2024-12-06T22:10:06.176733Z",
     "shell.execute_reply.started": "2024-12-06T22:10:04.634790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `hf_mistral` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `hf_mistral`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:18:02.819396Z",
     "iopub.status.busy": "2024-12-06T22:18:02.819036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc842abf04940a9a53a0fd247988725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Ask your questions (type 'exit' to quit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary distinction between machine learning and traditional statistics in terms of their end goals? A) ML focuses on model interpretability while statistics focuses on predictions B) ML emphasizes making predictions and decisions while statistics aims to find well-fitting models C) Statistics deals with small datasets while ML handles only big data D) ML is purely theoretical while statistics is application-oriented.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in section 1.1, demonstrates the practical focus of machine learning compared to the theoretical emphasis of statistics. While statistics seeks to understand data through model fitting, ML uses these models as means to an end for making predictions and decisions in real-world applications. This fundamental difference in focus shapes how we approach problem-solving in ML compared to traditional statistical analysis.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which of the following best describes a key characteristic of regression problems? A) The target values must be discrete and finite B) The output values are real numbers C) The input data must be one-dimensional D) The training data doesn't require paired examples. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 1.1.1, demonstrates that regression problems require continuous output values, unlike classification where outputs are discrete categories. The text specifically uses the example of predicting house prices, where the output values can range from $0 to $1,000,000. This continuous nature distinguishes regression from classification tasks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which of the following tasks would a data scientist use when trying to visualize patterns in high-dimensional customer purchase data? A) Classification B) Density Estimation C) Dimensionality Reduction D) Sequence Learning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the dimensionality reduction section, demonstrates how techniques like t-SNE and PCA help visualize patterns in high-dimensional data by reducing the dimensionality while preserving important patterns. This is particularly useful when dealing with large datasets like customer purchase history where visualizing patterns directly becomes computationally infeasible. The other tasks mentioned (classification, density estimation, and sequence learning) serve different purposes in machine learning.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which learning approach would be most suitable when you have access to a large dataset of unlabeled images but want to minimize the cost of human annotation? A) Reinforcement Learning B) Active Learning C) Semi-supervised Learning D) Transfer Learning. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the semi-supervised learning section, demonstrates how we can leverage both labeled and unlabeled data to improve model performance. The approach uses the unlabeled data to learn patterns and relationships in the data, while the labeled data is used to fine-tune the model's predictions. This approach is particularly effective when labeling data is expensive or time-consuming, making it ideal for scenarios where we have access to large amounts of unlabeled data. The course material shows this through the use of self-attention mechanisms in transformers, where the model can learn from both labeled (x⁽ⁱ⁾,y⁽ⁱ⁾) pairs and unlabeled x⁽ⁱ⁾ data.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the key distinction between parametric and non-parametric machine learning approaches? A) Parametric models require data preprocessing, while non-parametric models don't B) Non-parametric models make predictions directly from training data without learning parameters, while parametric models fit a model with parameters first C) Parametric models are always more accurate than non-parametric models D) Non-parametric models can only be used for classification tasks, while parametric models work for both classification and regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 1.1.1, demonstrates the fundamental difference in approach between these two types of ML methods. Parametric models (like linear regression) first learn parameters from the training data and then make predictions, while non-parametric models (like k-nearest neighbors) use the training data directly for prediction without learning parameters. This distinction is crucial for understanding how different ML algorithms work and when to use them.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What describes the primary challenge when using training error as the sole criterion for model selection? A) Training error is computationally expensive to calculate B) Training error may lead to overfitting and poor generalization C) Training error requires knowledge of the underlying data distribution D) Training error cannot be used with parameterized models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in section 2.7.1, demonstrates why we need additional criteria beyond training error for model selection. The text explains that while minimizing training error is important, it's not sufficient because it can lead to models that perform well on training data but poorly on new data (overfitting). This is particularly problematic when we want to make predictions outside of our training set, which is the ultimate goal of machine learning. The text introduces the concept of validation error as a more robust criterion for model selection.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What best describes the process of handling non-numerical real-world inputs in machine learning models? A) Converting inputs directly into predictions B) Using a feature function Ï† to transform inputs into real-valued vectors C) Limiting inputs to only numerical data D) Randomly assigning vector values to inputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 1.2.1, demonstrates how we handle non-numerical real-world inputs using feature functions Ï†(x) to transform them into real-valued vectors that can be used in machine learning models. This transformation is crucial because most ML algorithms require numerical inputs. The feature function Ï†(x) takes an arbitrary real-world object (like a color or a word) and converts it into a numerical representation that can be processed by the model. For example, Ï†(x) might transform a color into RGB values or a word into a one-hot encoding. This process is fundamental to applying ML to real-world problems where data isn't naturally numerical.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of comparing training error and test error in machine learning models?  A) To determine the computational efficiency of the model B) To evaluate how well the model performs on seen vs. unseen data C) To calculate the optimal number of training iterations D) To measure the speed of model convergence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, fundamental to model validation, demonstrates the model's generalization ability by comparing performance on training data (which the model has seen) versus test data (which it hasn't). The gap between these errors indicates how well the model is likely to perform on unseen data. This distinction is crucial for understanding a model's real-world effectiveness.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which of the following best describes the purpose of including a regularizer term in a machine learning objective function?  A) To minimize the prediction errors on training data only B) To ensure the model performs equally well on all types of data C) To balance between fitting training data and maintaining generalization ability D) To maximize the complexity of the model for better accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 2.6.1, demonstrates how regularization helps maintain generalization ability by preventing overfitting to training data. The regularizer term ‖Θ‖² is added to the objective function J(Θ) to balance between fitting the training data and maintaining the model's ability to generalize to unseen data. The text shows this through the trade-off between J_train(Θ) and J_test(Θ), where regularization helps minimize the gap between these two metrics. This concept is crucial for practical machine learning applications where we want our models to perform well on both training and test data.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary advantage of using squared loss in linear regression? A) It penalizes negative errors more heavily than positive errors B) It treats positive and negative prediction errors equally C) It works best with non-Gaussian noise distributions D) It only penalizes errors above a certain threshold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, fundamental to least squares regression, demonstrates the principle of equal error treatment. The squared loss function L(g,a) = (g-a)² ensures that both overestimation (g > a) and underestimation (g < a) are penalized equally, making it particularly effective for Gaussian noise distributions. This symmetry is crucial for minimizing the overall error in the presence of both positive and negative deviations from the true values.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When using matrix operations to solve for the optimal parameters in linear regression, what is the primary advantage of expressing the gradient as a column vector? A) It makes the calculations more complicated B) It allows for easier differentiation of individual elements C) It enables compact representation and efficient matrix operations D) It requires more memory storage than calculating derivatives separately\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 3.4.1, demonstrates how expressing the gradient as a column vector ∇Θᵀᵀ enables efficient matrix operations through the formula ∇Θᵀᵀ = 2(X̃ᵀX̃/n), where X̃ is the normalized matrix. This compact representation simplifies the optimization process while maintaining computational efficiency.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When solving for the optimal parameters in linear regression using the normal equation, which matrix operation is crucial to ensure the solution exists? A) Matrix addition of X transpose and Y B) Matrix multiplication of X and its transpose C) Matrix inversion of (X^T X) D) Matrix determinant of X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 3.4.1, demonstrates why we need the matrix to be invertible for the normal equation solution to exist. The derivation shows that θ = (X^T X)^{-1}X^T Y requires a non-singular matrix (X^T X), which ensures a unique solution for θ.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary reason for using regularization in machine learning models? A) To increase the training loss B) To achieve perfect fit on training data C) To help the model generalize better to unseen data D) To make computations faster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 2.6.1, demonstrates how regularization helps prevent overfitting by adding a penalty term to the loss function. The regularization term encourages the model to find solutions that generalize well to unseen data, rather than just fitting the training data perfectly. This is crucial because we want our model to perform well on new, unseen data, not just on the training set. The text shows this through the introduction of the regularization parameter λ, which balances the trade-off between fitting the training data and generalizing to new data.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of adding the regularization term Î»||Î¸||Â² in ridge regression? A) To make the computation faster B) To ensure the model perfectly fits all training points C) To prevent overfitting and control the magnitude of model parameters D) To guarantee that X^TX is always invertible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: D. This concept, covered in the regularization section, demonstrates how the regularization term helps prevent overfitting by controlling the magnitude of model parameters. The term Î»||Î¸||Â² penalizes large parameter values, encouraging the model to find solutions that generalize well to unseen data rather than just fitting the training data too closely. This is crucial for real-world applications where we want our model to perform well on new data rather than just optimizing training metrics.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary reason for having separate training and test error measurements in machine learning? A) To compare different machine learning algorithms B) To measure the computational efficiency of the model C) To evaluate the model's performance on unseen data and avoid overfitting D) To determine the optimal learning rate for gradient descent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the introduction to evaluation metrics, demonstrates why we need both training error En(h; Θ) and test error E(h) in machine learning. The training error measures the algorithm's performance on the training data, while the test error quantifies how well the algorithm generalizes to unseen data. This distinction is crucial for understanding a model's real-world performance and avoiding overfitting to the training data.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of using validation in evaluating machine learning algorithms?  A) To speed up the training process B) To determine the optimal number of features C) To assess algorithm performance on data independent from training D) To reduce memory requirements during training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 2.7.1, demonstrates the importance of evaluating algorithm performance on independent data. The validation set, separate from the training set, provides an unbiased estimate of algorithm performance on unseen data, which is crucial for understanding model generalization. This distinction is crucial because while training error (En(h; Θ)) measures algorithm performance on the training data, validation error (E(h)) provides a more reliable estimate of real-world performance. The text shows this through the comparison of En(h; Θ) and E(h) on the same dataset, demonstrating how validation helps bridge the gap between training and test error. This concept is fundamental to understanding model generalization and performance in practice.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When evaluating different machine learning approaches, what does cross-validation primarily assess? A) The performance of a specific hypothesis B) The effectiveness of the learning algorithm itself C) The quality of individual data points D) The size of the validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 2.7, demonstrates how cross-validation is used to assess the stability and robustness of learning algorithms by averaging performance across multiple training/test splits. It's particularly valuable when we have limited data, as it provides a more reliable estimate of algorithm performance than single-run evaluations.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which stopping criterion would be most reliable for gradient descent when the objective function has multiple local minima?  A) Stopping after a fixed number of iterations T B) Stopping when the change in parameter value is less than epsilon C) Stopping when the derivative at current theta is less than epsilon D) Stopping when the change in function value is less than epsilon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: D. This concept, covered in the convergence criteria section, demonstrates why the function value change (f(θ⁽ᵗ⁾ - f(θ⁽ᵗ⁻¹⁾)) < ϵ) is preferred over parameter change (||θ⁽ᵗ⁾ - θ⁽ᵗ⁻¹⁾|| < ϵ) or derivative magnitude (|f'(θ⁽ᵗ⁾)| < ϵ) for convergence detection. The function value change monotonically decreases towards zero as we approach a local minimum, making it a more reliable indicator of proximity to a stationary point. This is particularly important when dealing with non-convex functions where multiple local minima may exist.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What happens when gradient descent is applied to a non-convex function?  A) It always converges to a global minimum regardless of the starting point B) It always converges to the nearest local minimum C) It may converge to a point where the gradient is zero but is not necessarily a minimum D) It never converges and always oscillates around multiple minima\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 3.1.1, demonstrates the fundamental limitation of gradient descent when dealing with non-convex functions. The algorithm can converge to a point where the gradient is zero, but this point might not be a minimum. The convergence depends on the starting point x_init and the parameters η and ε, making the algorithm's behavior unpredictable for non-convex functions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the main difference in handling the bias term (Î¸â‚€) between ordinary least squares and ridge regression? A) In ridge regression, Î¸â‚€ must be handled separately from Î¸, while in OLS they can be combined B) OLS requires separate handling of Î¸â‚€, while ridge regression combines it with Î¸ C) Both OLS and ridge regression handle Î¸â‚€ separately D) Neither OLS nor ridge regression distinguish between Î¸â‚€ and Î¸\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: A. This concept, covered in Section 3.3.1, demonstrates how ridge regression handles bias differently than ordinary least squares. While OLS combines θ and θ₀ into a single parameter vector θ₀, ridge regression treats θ₀ as a separate bias term. The text shows this by keeping θ₀ separate from θ in the ridge regression update equations. This distinction allows ridge regression to balance the trade-off between structural and estimation error while maintaining the interpretability of the bias term.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary advantage of using stochastic gradient descent over traditional gradient descent? A) It always converges to the global minimum B) It uses less memory since it processes one data point at a time  C) It requires more iterations to converge D) It must process the entire dataset before making parameter updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 3.4.1, demonstrates how SGD processes individual data points rather than the entire dataset at once, making it particularly useful for large datasets. The memory advantage comes at the cost of potentially slower convergence compared to batch gradient descent.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is a key advantage of using a decreasing step size sequence in Stochastic Gradient Descent? A) It allows for faster convergence to global optima in all cases B) It balances exploration in early stages while ensuring convergence in later stages C) It eliminates the need for randomization in gradient selection D) It guarantees convergence regardless of the initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in the SGD convergence analysis, demonstrates how the adaptive learning rate balances exploration in early stages while ensuring convergence in later stages. The text shows that with η(t)=η₀/t, the algorithm achieves both exploration (when t is large) and fast convergence (when t is small). This balance is crucial because it allows the algorithm to explore the parameter space effectively while ensuring eventual convergence to near-optimal points. This makes SGD particularly effective for noisy objective functions where exploration is necessary but convergence is desired. The text shows this through the convergence analysis where ‖f(x⁽ᵗ⁾-f⁽ᵗ⁻¹⁾‖₂/‖f⁽ᵗ⁻¹⁾‖₂ converges to zero as t→∞.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the key difference between the evaluation metrics used for classification and regression tasks?  A) Classification always uses mean squared error while regression uses 0-1 loss B) Classification commonly uses 0-1 loss that counts misclassifications, while regression typically uses continuous error measures C) Both classification and regression must use the same error metrics to be comparable D) Classification error can only be measured on training data, not test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in the evaluation metrics section, demonstrates the fundamental difference in how we measure performance between classification and regression tasks. While regression typically uses continuous error measures like mean squared error (MSE), classification often uses 0-1 loss (also known as NLL loss) which counts misclassifications. This distinction is crucial because it reflects the different nature of the prediction spaces: regression predictions are continuous, while classification predictions are discrete categories. The choice of evaluation metric directly influences how we interpret model performance and optimize model parameters.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What does the normal vector Î¸ indicate in relation to a linear classifier? A) It points parallel to the decision boundary B) It points perpendicular to the decision boundary C) It has no geometric relationship to the decision boundary D) It represents the angle between two classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 4.3.1, demonstrates how the normal vector is perpendicular to the separator (decision boundary). This geometric relationship is crucial because it defines the orientation of the hyperplane that separates the two classes. The normal vector Î¸⁽ᵗ⁾ = [a, b]ᵀ, where a and b are the coefficients from the equation θᵀx + θ₀ = 0, indicates the direction perpendicular to the separator. This geometric interpretation is fundamental to understanding how linear classifiers partition the feature space.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Why is optimizing a linear classifier using 0-1 loss function computationally challenging? A) The optimization problem requires complex matrix operations B) The loss function is discontinuous and non-smooth, making it hard to determine parameter improvements C) The computational time is always linear with the number of training examples D) The classifier can only make binary predictions of +1 or -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. The 0-1 loss function's discontinuity and non-smoothness make it difficult to determine optimal parameters through gradient descent or other optimization methods. This contrasts with NLL loss, which is continuous and differentiable, making optimization more tractable.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is a key characteristic of the sigmoid function that makes it suitable for probability-based predictions? A) It always outputs values between -1 and +1 B) It always outputs values between 0 and 1 C) It always outputs values greater than 1 D) It always outputs negative values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. The sigmoid function σ(z) = 1/(1 + e^(-z)) maps any real number to the range (0,1), making it ideal for probability-based predictions. This property is particularly useful in binary classification tasks where we need probabilistic outputs between 0 and 1 for both classes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary advantage of using negative log-likelihood (NLL) as a loss function in classification problems? A) It simplifies the computation of gradients during optimization B) It naturally extends to multi-class classification scenarios C) It always produces loss values between 0 and 1 D) It forces the model parameters to be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 4.2.1, demonstrates how NLL loss can handle multiple classes through the use of one-hot encoding in y⁽ᵗ⁾. This makes it particularly suitable for multi-class classification tasks, unlike some other loss functions that are inherently binary.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of the regularization term (2Î»Î¸) in the gradient computation for logistic regression? A) To speed up the convergence of gradient descent B) To ensure the loss function is differentiable C) To prevent overfitting by controlling parameter magnitudes D) To normalize the input features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the regularization section, demonstrates how the regularization term helps prevent overfitting by controlling parameter magnitudes through the L2 norm. The term 2Î»Î¸ in the gradient computation serves as a constraint that pulls parameters toward zero when Î» is large enough, preventing individual features from dominating the model. This regularization effect is crucial for generalization in machine learning models.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What property ensures that gradient descent will find the global minimum in logistic regression's negative log-likelihood loss function? A) The second derivative is always positive B) The first derivative is monotonically increasing C) The sigmoid function is bounded between 0 and 1 D) The loss function has multiple local minima\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the logistic regression optimization section, demonstrates why gradient descent converges to near-optimal parameters. The monotonically increasing nature of the sigmoid function's derivative ensures that each update moves us closer to the global minimum. This is crucial because the negative log-likelihood loss function has multiple local minima, making convergence to the global minimum non-guaranteed without this property.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of using the softmax function in multi-class classification? A) To convert input features into binary values B) To normalize input vectors to unit length C) To transform arbitrary real-valued vectors into probability distributions D) To reduce the dimensionality of the input space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the multi-class classification section, demonstrates how softmax transforms arbitrary real-valued vectors into valid probability distributions. The function takes a vector z ∈ ℝᵈ and outputs values that are non-negative and sum to 1, making it ideal for classification tasks where we need interpretable class probabilities. The exponential terms in softmax(z) = [exp(z₁)/Σexp(zᵢ), ..., exp(zₖ)/Σexp(zᵢ)]ᵀ ensure these properties.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is a key benefit of applying non-linear feature transformations to input data? A) It reduces computational complexity B) It eliminates the need for validation C) It allows linear models to capture non-linear patterns D) It decreases the number of required training examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 4.2.1, demonstrates how non-linear transformations enable linear models to capture complex patterns by mapping the input space to a new feature space where linear separation becomes possible. The text uses the example of transforming 1D data using φ(x) = [x, x²]ᵀ, enabling linear separation in the transformed space while capturing non-linear patterns in the original space. This is crucial because many real-world patterns are inherently non-linear, and applying non-linear transformations enables linear models to capture these patterns effectively.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When transforming input features to better represent patterns in data, which approach uses exponential decay based on distance from reference points? A) Polynomial basis transformation B) Linear basis mapping C) Radial basis functions D) Logarithmic basis functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, fundamental to radial basis functions (RBFs), demonstrates how the transformation captures local patterns through functions centered around reference points. The exponential decay property ensures that patterns far from the reference points contribute less to the transformation, creating a smooth transition between local patterns. This makes RBFs particularly effective for capturing complex patterns with local structure. The mathematical formulation uses $f_p(x) = e^{-\\beta\\|p-x\\|^2}$ where $p$ is the reference point and $\\beta$ controls the decay rate. This approach contrasts with polynomial basis transformations, which capture global patterns through polynomial expansions, and linear basis mapping, which maintains linear relationships between transformed features.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When working with categorical variables in a machine learning model, which encoding method uses a series of binary variables where each subsequent value includes all previous ones set to 1? A) One-hot encoding B) Binary encoding C) Thermometer encoding D) Numeric encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the encoding methods section, demonstrates how thermometer encoding creates a hierarchical binary representation where each additional value includes all previous ones set to 1. This makes it particularly useful for ordinal data where the relationship between categories matters. The example given in the notes shows how 3 becomes 111 in binary, representing both 2 and 3 at once. This encoding maintains the ordinal relationship while preserving the binary nature that's beneficial for some machine learning algorithms.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What correctly describes the basic computational operation in a neural network's single layer? A) The output is computed as a linear combination of inputs multiplied by a bias term B) The activation is calculated as f(W^TX + W_0), where f is a non-linear function applied to the weighted sum plus bias C) The output is simply the dot product of weights and inputs D) The activation is computed by multiplying the input matrix with a bias vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in Section 6.2.1, demonstrates the fundamental building block of neural networks. The layer's operation involves computing W^TX + W_0, where W is the matrix of weights, X is the input matrix, and W_0 is the bias term. The activation function f is then applied to this weighted sum to produce the output A^L. This formulation allows for both linear (when f(z) = z) and non-linear transformations, making it versatile for various tasks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What would happen in a neural network if all activation functions were replaced with the identity function (f(x) = x)? A) The network would still maintain its non-linear modeling capabilities B) The network would reduce to a single linear transformation equivalent to multiplying all weight matrices C) The network would require more hidden layers to compensate D) The network would perform better on regression tasks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in Section 6.3.1, demonstrates why we need non-linear activation functions. When all activation functions are replaced with the identity function, the entire multi-layer network collapses into a single linear transformation (W_L^T W_{L-1}^T...W_1^T X), losing the ability to model complex non-linear relationships. This is why we need activation functions to introduce non-linearity back into the network.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which activation function would be most appropriate for a neural network's final layer if the task is to classify images into exactly one of several categories? A) Sigmoid function B) Linear function C) Softmax function D) Hyperbolic tangent function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. The softmax function is specifically designed for multi-class classification tasks, where the output is a vector of probabilities summing to 1. This makes it ideal for the final layer of a neural network when the goal is to assign an image to one of several categories. The sigmoid function, while useful for binary classification, cannot produce a vector of probabilities for multiple classes. The linear function is not suitable for classification tasks as it doesn't naturally produce probability distributions. The hyperbolic tangent function, while commonly used in hidden layers, is not typically used in the final layer for classification tasks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the key component in computing the gradient of the loss with respect to weights in a neural network layer? A) Using rectified linear units (ReLU) as activation functions B) Breaking down matrix equations into scalar components and applying chain rule C) Multiplying all weight matrices in reverse order D) Computing only the final layer derivatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in the backpropagation section, demonstrates how we break down matrix equations into scalar components and apply the chain rule to compute gradients for all weights in the network. The process involves systematically computing ∂Z^l/∂W^l, ∂A^l/∂Z^l, ∂loss/∂A^l, and ∂loss/∂W^l for each layer l, where A^l represents the activations and Z^l represents the pre-activations. This systematic approach allows us to compute gradients for all weights in the network, not just the final layer weights. This is crucial for training the network effectively.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the primary purpose of computing partial derivatives with respect to weights during neural network training? A) To determine the optimal number of hidden layers B) To calculate how much each weight contributes to the overall loss C) To normalize the input data D) To initialize the weights randomly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, fundamental to backpropagation, shows how each weight's adjustment affects the loss function, enabling efficient gradient descent optimization.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Why is it crucial to initialize neural network weights with small random values rather than all zeros? A) To ensure faster convergence of the network during training B) To prevent all neurons from learning the same features and break symmetry C) To avoid saturation of activation functions in deep layers D) All of the above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: D. This concept, covered in Section 6.3.1, demonstrates why random initialization is fundamental to neural network functionality. The text explains that zero initialization would cause all neurons to learn the same features due to symmetry breaking, but more importantly, it would prevent the network from learning complex patterns by saturating activation functions in deep layers. The random initialization ensures each neuron starts with unique weights, allowing the network to learn different aspects of the input space.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When training neural networks, which of the following represents an effective strategy to prevent overfitting? A) Increasing the learning rate exponentially during training B) Maximizing the gradient values during backpropagation C) Early stopping based on validation set performance D) Removing all activation functions to simplify the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the regularization section, demonstrates how early stopping can help prevent overfitting by terminating training when validation performance starts to degrade. This approach is particularly effective when combined with other regularization techniques like weight decay. The other options either accelerate training (A) or simplify the model (D) in ways that don't directly address overfitting. Maximizing gradient values during backpropagation (B) is actually counterproductive as it would lead to larger weights and potentially worse generalization.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which technique randomly deactivates neurons during training to prevent overfitting while requiring weight adjustments during prediction? A) Batch normalization B) Mini-batch gradient descent C) Dropout D) Weight decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. Dropout is specifically designed to prevent overfitting by randomly deactivating neurons during training while maintaining the network's structure. This forces the network to learn more robust features that generalize well to unseen data. The technique involves multiplying the input by 1-p during training, where p is the dropout rate, and then performing the computation as usual. During prediction, all neurons are active, ensuring optimal performance. This technique is particularly effective when combined with other regularization methods like weight decay.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which characteristic best describes why we don't need to analyze pixels from distant corners of an image when detecting local features? A) Translation invariance B) Spatial locality C) Filter dimensionality D) RGB channel separation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, fundamental to local feature detection, demonstrates how local patterns repeat regardless of their position in the image, making it unnecessary to analyze distant corners. Spatial locality ensures that patterns observed in one part of the image are likely to occur elsewhere, enabling efficient detection while maintaining robustness to translation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When applying multiple filters to an image in a convolutional neural network, what happens to the dimensionality of the output? A) The output becomes a 1D vector B) The output maintains the same dimensions as the input image C) The output becomes a 3D tensor with depth equal to the number of filters D) The output reduces in size based on the number of filters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: C. This concept, covered in the CNN architecture section, demonstrates how each filter produces a new channel in the output. When k filters are applied to an n×n input image, the result is an n×n×k tensor. This hierarchical representation enables the network to learn increasingly complex features across multiple channels.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  When computing gradients for a neural network with a convolutional layer followed by ReLU activation, which component ensures that updates only occur for activated neurons?  A) The partial derivative of ZÂ¹ with respect to WÂ¹ B) The diagonal matrix where elements are 1 for positive inputs and 0 otherwise C) The transpose of the weight matrix WÂ² D) The squared loss function derivative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "==================================================\n",
      "Answer: Answer: B. This concept, covered in the backpropagation section, demonstrates how gradient descent selectively updates weights based on the activation function's output. The diagonal matrix (D) where elements are 1 for positive inputs and 0 otherwise represents the derivative of the ReLU function, ensuring that updates only occur for activated neurons. This mechanism is crucial for efficient training as it prevents unnecessary weight adjustments for neurons that didn't contribute to the loss function's gradient.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_model():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, \"/kaggle/input/finetuned/pytorch/default/1/final\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    prompt = f\"\"\"<s>[INST]@ESE577. For multiple choice questions:\n",
    "    1. Just give the correct option as \"Answer: [letter]\"\n",
    "    Question: {question} [/INST]\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=40,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "def chat():\n",
    "   print(\"Loading model...\")\n",
    "   model, tokenizer = load_model()\n",
    "   print(\"Model loaded! Ask your questions (type 'exit' to quit)\")\n",
    "   \n",
    "   while True:\n",
    "       question = input(\"\\nQuestion: \")\n",
    "       if question.lower() == 'exit':\n",
    "           break\n",
    "       try:\n",
    "           response = generate_response(model, tokenizer, question)\n",
    "           print(\"\\nResponse:\")\n",
    "           print(\"=\" * 50)\n",
    "           print(response)\n",
    "       except Exception as e:\n",
    "           print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 185157,
     "modelInstanceId": 162799,
     "sourceId": 190994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

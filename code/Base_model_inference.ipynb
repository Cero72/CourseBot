{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T23:41:54.998570Z",
     "iopub.status.busy": "2024-12-06T23:41:54.997858Z",
     "iopub.status.idle": "2024-12-06T23:41:57.151475Z",
     "shell.execute_reply": "2024-12-06T23:41:57.150628Z",
     "shell.execute_reply.started": "2024-12-06T23:41:54.998527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `hf_mistral` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `hf_mistral`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T23:41:57.153078Z",
     "iopub.status.busy": "2024-12-06T23:41:57.152783Z",
     "iopub.status.idle": "2024-12-07T00:55:34.256631Z",
     "shell.execute_reply": "2024-12-07T00:55:34.255285Z",
     "shell.execute_reply.started": "2024-12-06T23:41:57.153033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952918a0877f4d1483341bde3ec25dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f32495580f44c9b7722777ecb8a16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e18bba4ac8c4744b6c0e24be1144707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f25792367845f188e47f4e458cf135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12d4e3a75dc49f8a63a14e208c3f0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95d97a54fd9483b88d05fb764835d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7070d407049445196f1cd52d0d8b927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d5b9129d9543f991a5d485c47dc119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b6ad4e45d34621a211c319dcd23cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6691f4cec947d788795925c4f45653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda07e34d1774ee694d7c750c0163b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb6a08156234d3a8383be41f4505b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary distinction between machine learning and traditional statistics in terms of their end goals? A) ML focuses on model interpretability while statistics focuses on predictions B) ML emphasizes making predictions and decisions while statistics aims to find well-fitting models C) Statistics deals with small datasets while ML handles only big data D) ML is purely theoretical while statistics is application-oriented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. Statistics aims to find well-fitting models, while ML emphasizes making predictions and decisions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which of the following best describes a key characteristic of regression problems? A) The target values must be discrete and finite B) The output values are real numbers C) The input data must be one-dimensional D) The training data doesn't require paired examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. The input data must be one-dimensional\n",
      "\n",
      "(Note: Regression problems involve predicting continuous output values based on one or more input features. Therefore, the input data is typically one-dimensional, meaning it can be represented as a single feature vector.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which of the following tasks would a data scientist use when trying to visualize patterns in high-dimensional customer purchase data? A) Classification B) Density Estimation C) Dimensionality Reduction D) Sequence Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Dimensionality Reduction\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which learning approach would be most suitable when you have access to a large dataset of unlabeled images but want to minimize the cost of human annotation? A) Reinforcement Learning B) Active Learning C) Semi-supervised Learning D) Transfer Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Semi-supervised Learning\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the key distinction between parametric and non-parametric machine learning approaches? A) Parametric models require data preprocessing, while non-parametric models don't B) Non-parametric models make predictions directly from training data without learning parameters, while parametric models fit a model with parameters first C) Parametric models are always more accurate than non-parametric models D) Non-parametric models can only be used for classification tasks, while parametric models work for both classification and regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. Parametric models can be more accurate than non-parametric models.\n",
      "A. Parametric models require learning parameters from data, while non-parametric models do not.\n",
      "B. Parametric models can be used for both classification and regression tasks, while non-parametric models can be used for both as well.\n",
      "D. Both parametric and non-parametric models can handle both classification and regression tasks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What describes the primary challenge when using training error as the sole criterion for model selection? A) Training error is computationally expensive to calculate B) Training error may lead to overfitting and poor generalization C) Training error requires knowledge of the underlying data distribution D) Training error cannot be used with parameterized models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Training error may lead to overfitting and poor generalization\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What best describes the process of handling non-numerical real-world inputs in machine learning models? A) Converting inputs directly into predictions B) Using a feature function Ï† to transform inputs into real-valued vectors C) Limiting inputs to only numerical data D) Randomly assigning vector values to inputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The correct answer is B) Using a feature function Ï† to transform inputs into real-valued vectors.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of comparing training error and test error in machine learning models?  A) To determine the computational efficiency of the model B) To evaluate how well the model performs on seen vs. unseen data C) To calculate the optimal number of training iterations D) To measure the speed of model convergence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) To evaluate how well the model performs on seen vs. unseen data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which of the following best describes the purpose of including a regularizer term in a machine learning objective function?  A) To minimize the prediction errors on training data only B) To ensure the model performs equally well on all types of data C) To balance between fitting training data and maintaining generalization ability D) To maximize the complexity of the model for better accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. To balance between fitting training data and maintaining generalization ability\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary advantage of using squared loss in linear regression? A) It penalizes negative errors more heavily than positive errors B) It treats positive and negative prediction errors equally C) It works best with non-Gaussian noise distributions D) It only penalizes errors above a certain threshold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. It works best with non-Gaussian noise distributions\n",
      "\n",
      "Note: In linear regression, squared loss (also known as L2 loss or mean squared error) is a common choice for the loss function. It measures the difference between the predicted values and the true values by calculating the squared difference between them for each observation. The primary advantage of using squared loss is that it is well-suited for regression problems with non-Gaussian noise distributions. This is because squared loss is more robust to outliers and heavy-tailed errors compared to other loss functions like absolute error (L1 loss) or Huber loss. However, it's important to note that the choice of loss function depends on the specific characteristics of the data and the problem at hand.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When using matrix operations to solve for the optimal parameters in linear regression, what is the primary advantage of expressing the gradient as a column vector? A) It makes the calculations more complicated B) It allows for easier differentiation of individual elements C) It enables compact representation and efficient matrix operations D) It requires more memory storage than calculating derivatives separately\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. It enables compact representation and efficient matrix operations.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When solving for the optimal parameters in linear regression using the normal equation, which matrix operation is crucial to ensure the solution exists? A) Matrix addition of X transpose and Y B) Matrix multiplication of X and its transpose C) Matrix inversion of (X^T X) D) Matrix determinant of X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Matrix inversion of (X^T X)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary reason for using regularization in machine learning models? A) To increase the training loss B) To achieve perfect fit on training data C) To help the model generalize better to unseen data D) To make computations faster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The primary reason for using regularization in machine learning models is to help the model generalize better to unseen data (Option C). Regularization techniques, such as L1 and L2 regularization, add a penalty term to the loss function to prevent overfitting and improve the model's ability to generalize to new data.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of adding the regularization term Î»||Î¸||Â² in ridge regression? A) To make the computation faster B) To ensure the model perfectly fits all training points C) To prevent overfitting and control the magnitude of model parameters D) To guarantee that X^TX is always invertible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) To prevent overfitting and control the magnitude of model parameters\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary reason for having separate training and test error measurements in machine learning? A) To compare different machine learning algorithms B) To measure the computational efficiency of the model C) To evaluate the model's performance on unseen data and avoid overfitting D) To determine the optimal learning rate for gradient descent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) To evaluate the model's performance on unseen data and avoid overfitting\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of using validation in evaluating machine learning algorithms?  A) To speed up the training process B) To determine the optimal number of features C) To assess algorithm performance on data independent from training D) To reduce memory requirements during training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The primary purpose of using validation in evaluating machine learning algorithms is to assess algorithm performance on data independent from training (Option C).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When evaluating different machine learning approaches, what does cross-validation primarily assess? A) The performance of a specific hypothesis B) The effectiveness of the learning algorithm itself C) The quality of individual data points D) The size of the validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "A) The performance of a specific hypothesis\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which stopping criterion would be most reliable for gradient descent when the objective function has multiple local minima?  A) Stopping after a fixed number of iterations T B) Stopping when the change in parameter value is less than epsilon C) Stopping when the derivative at current theta is less than epsilon D) Stopping when the change in function value is less than epsilon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. Stopping when the derivative at current theta is less than epsilon\n",
      "\n",
      "However, it's important to note that this criterion may not guarantee finding the global minimum, but it can help prevent oscillations and improve convergence. The other options may not be reliable when dealing with multiple local minima.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What happens when gradient descent is applied to a non-convex function?  A) It always converges to a global minimum regardless of the starting point B) It always converges to the nearest local minimum C) It may converge to a point where the gradient is zero but is not necessarily a minimum D) It never converges and always oscillates around multiple minima\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "D. It may converge to a point where the gradient is zero but is not necessarily a minimum, or it may get stuck at a local minimum, or it may never converge and oscillate around multiple minima. In the case of a non-convex function, there is no guarantee that gradient descent will converge to the global minimum, unlike in the case of a convex function.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the main difference in handling the bias term (Î¸â‚€) between ordinary least squares and ridge regression? A) In ridge regression, Î¸â‚€ must be handled separately from Î¸, while in OLS they can be combined B) OLS requires separate handling of Î¸â‚€, while ridge regression combines it with Î¸ C) Both OLS and ridge regression handle Î¸â‚€ separately D) Neither OLS nor ridge regression distinguish between Î¸â‚€ and Î¸\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "A) In ridge regression, Î¸â‚€ must be handled separately from Î¸, while in OLS they can be combined.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary advantage of using stochastic gradient descent over traditional gradient descent? A) It always converges to the global minimum B) It uses less memory since it processes one data point at a time  C) It requires more iterations to converge D) It must process the entire dataset before making parameter updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "B. It uses less memory since it processes one data point at a time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is a key advantage of using a decreasing step size sequence in Stochastic Gradient Descent? A) It allows for faster convergence to global optima in all cases B) It balances exploration in early stages while ensuring convergence in later stages C) It eliminates the need for randomization in gradient selection D) It guarantees convergence regardless of the initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. It balances exploration in early stages while ensuring convergence in later stages.\n",
      "\n",
      "Explanation: A decreasing step size sequence in Stochastic Gradient Descent (SGD) is also known as learning rate schedule. It helps in balancing exploration and exploitation during the training process. In the early stages of training, a larger learning rate is used to explore the search space and escape from local minima. As the training progresses, the learning rate is decreased to ensure convergence to the global minimum. This way, SGD can effectively explore the search space while ensuring convergence to the optimal solution. It does not eliminate the need for randomization in gradient selection, nor does it guarantee convergence regardless of the initial parameters.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the key difference between the evaluation metrics used for classification and regression tasks?  A) Classification always uses mean squared error while regression uses 0-1 loss B) Classification commonly uses 0-1 loss that counts misclassifications, while regression typically uses continuous error measures C) Both classification and regression must use the same error metrics to be comparable D) Classification error can only be measured on training data, not test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Classification commonly uses 0-1 loss that counts misclassifications, while regression typically uses continuous error measures.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What does the normal vector Î¸ indicate in relation to a linear classifier? A) It points parallel to the decision boundary B) It points perpendicular to the decision boundary C) It has no geometric relationship to the decision boundary D) It represents the angle between two classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The answer is: B. It points perpendicular to the decision boundary.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Why is optimizing a linear classifier using 0-1 loss function computationally challenging? A) The optimization problem requires complex matrix operations B) The loss function is discontinuous and non-smooth, making it hard to determine parameter improvements C) The computational time is always linear with the number of training examples D) The classifier can only make binary predictions of +1 or -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. The computational time is always linear with the number of training examples.\n",
      "\n",
      "However, it's important to note that while the computational time may be linear with the number of training examples, optimizing a linear classifier using a 0-1 loss function can still be computationally challenging for large datasets due to the need to evaluate the model on the entire dataset for each iteration of the optimization process. Additionally, the 0-1 loss function is not differentiable at the boundaries between classes, which can make optimization more difficult using gradient-based methods.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is a key characteristic of the sigmoid function that makes it suitable for probability-based predictions? A) It always outputs values between -1 and +1 B) It always outputs values between 0 and 1 C) It always outputs values greater than 1 D) It always outputs negative values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The correct answer is: B. It always outputs values between 0 and 1.\n",
      "\n",
      "Sigmoid function is commonly used in machine learning and deep learning models for activating neurons because it maps any input to a value between 0 and 1, which is suitable for probability-based predictions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary advantage of using negative log-likelihood (NLL) as a loss function in classification problems? A) It simplifies the computation of gradients during optimization B) It naturally extends to multi-class classification scenarios C) It always produces loss values between 0 and 1 D) It forces the model parameters to be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. It naturally extends to multi-class classification scenarios.\n",
      "\n",
      "Explanation:\n",
      "Negative log-likelihood (NLL) is commonly used as a loss function in classification problems, especially in probabilistic models like Naive Bayes, Logistic Regression, and Neural Networks. The primary advantage of using NLL as a loss function in classification problems is that it naturally extends to multi-class classification scenarios. In contrast, other loss functions like zero-one loss or hinge loss are only defined for binary classification problems. NLL allows us to model the probability distribution over all classes, making it a more flexible and powerful choice for multi-class classification problems.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of the regularization term (2Î»Î¸) in the gradient computation for logistic regression? A) To speed up the convergence of gradient descent B) To ensure the loss function is differentiable C) To prevent overfitting by controlling parameter magnitudes D) To normalize the input features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) To prevent overfitting by controlling parameter magnitudes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What property ensures that gradient descent will find the global minimum in logistic regression's negative log-likelihood loss function? A) The second derivative is always positive B) The first derivative is monotonically increasing C) The sigmoid function is bounded between 0 and 1 D) The loss function has multiple local minima\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) The sigmoid function is bounded between 0 and 1\n",
      "\n",
      "(Note: This property does not guarantee that gradient descent will find the global minimum, but it is a necessary condition for the global minimum to exist in logistic regression's negative log-likelihood loss function.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of using the softmax function in multi-class classification? A) To convert input features into binary values B) To normalize input vectors to unit length C) To transform arbitrary real-valued vectors into probability distributions D) To reduce the dimensionality of the input space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. To transform arbitrary real-valued vectors into probability distributions\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is a key benefit of applying non-linear feature transformations to input data? A) It reduces computational complexity B) It eliminates the need for validation C) It allows linear models to capture non-linear patterns D) It decreases the number of required training examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) It allows linear models to capture non-linear patterns\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When transforming input features to better represent patterns in data, which approach uses exponential decay based on distance from reference points? A) Polynomial basis transformation B) Linear basis mapping C) Radial basis functions D) Logarithmic basis functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Radial basis functions\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When working with categorical variables in a machine learning model, which encoding method uses a series of binary variables where each subsequent value includes all previous ones set to 1? A) One-hot encoding B) Binary encoding C) Thermometer encoding D) Numeric encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Binary encoding is the encoding method that uses a series of binary variables where each subsequent value includes all previous ones set to 1. However, it's important to note that this is not a common or recommended encoding method for categorical variables in machine learning, as it can lead to a large number of features and potential overfitting. One-hot encoding is the more commonly used method for encoding categorical variables in machine learning.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What correctly describes the basic computational operation in a neural network's single layer? A) The output is computed as a linear combination of inputs multiplied by a bias term B) The activation is calculated as f(W^TX + W_0), where f is a non-linear function applied to the weighted sum plus bias C) The output is simply the dot product of weights and inputs D) The activation is computed by multiplying the input matrix with a bias vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The correct answer is B) The activation is calculated as f(W^TX + W_0), where f is a non-linear function applied to the weighted sum plus bias.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What would happen in a neural network if all activation functions were replaced with the identity function (f(x) = x)? A) The network would still maintain its non-linear modeling capabilities B) The network would reduce to a single linear transformation equivalent to multiplying all weight matrices C) The network would require more hidden layers to compensate D) The network would perform better on regression tasks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. The network would require more hidden layers to compensate.\n",
      "\n",
      "Explanation: Replacing all activation functions with the identity function would make the neural network equivalent to a linear perceptron, which can only learn linear relationships between inputs and outputs. To learn non-linear relationships, a neural network needs at least one non-linear activation function in each hidden layer. Therefore, more hidden layers would be required to compensate for the lack of non-linearity in the activation functions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which activation function would be most appropriate for a neural network's final layer if the task is to classify images into exactly one of several categories? A) Sigmoid function B) Linear function C) Softmax function D) Hyperbolic tangent function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Softmax function. The softmax function is commonly used for the output layer of a neural network in multi-class classification problems, as it converts the output of the last layer into probabilities that sum up to 1, allowing the network to output a probability distribution over the classes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the key component in computing the gradient of the loss with respect to weights in a neural network layer? A) Using rectified linear units (ReLU) as activation functions B) Breaking down matrix equations into scalar components and applying chain rule C) Multiplying all weight matrices in reverse order D) Computing only the final layer derivatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Breaking down matrix equations into scalar components and applying chain rule.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  What is the primary purpose of computing partial derivatives with respect to weights during neural network training? A) To determine the optimal number of hidden layers B) To calculate how much each weight contributes to the overall loss C) To normalize the input data D) To initialize the weights randomly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. To calculate how much each weight contributes to the overall loss.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Why is it crucial to initialize neural network weights with small random values rather than all zeros? A) To ensure faster convergence of the network during training B) To prevent all neurons from learning the same features and break symmetry C) To avoid saturation of activation functions in deep layers D) All of the above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) To avoid saturation of activation functions in deep layers\n",
      "\n",
      "(Answer: C)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When training neural networks, which of the following represents an effective strategy to prevent overfitting? A) Increasing the learning rate exponentially during training B) Maximizing the gradient values during backpropagation C) Early stopping based on validation set performance D) Removing all activation functions to simplify the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) Early stopping based on validation set performance\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which technique randomly deactivates neurons during training to prevent overfitting while requiring weight adjustments during prediction? A) Batch normalization B) Mini-batch gradient descent C) Dropout D) Weight decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The correct answer is C) Dropout. Dropout is a regularization technique that randomly deactivates neurons during training to prevent overfitting, while requiring weight adjustments during prediction.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  Which characteristic best describes why we don't need to analyze pixels from distant corners of an image when detecting local features? A) Translation invariance B) Spatial locality C) Filter dimensionality D) RGB channel separation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "The correct answer is C) Spatial locality.\n",
      "\n",
      "Explanation: In computer vision, spatial locality refers to the property that pixels or features in an image that are close to each other are more likely to be similar than pixels or features that are far apart. Therefore, when detecting local features in an image, we don't need to analyze pixels from distant corners because the features in those areas are less likely to be relevant to the features we're trying to detect in the center of the image.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When applying multiple filters to an image in a convolutional neural network, what happens to the dimensionality of the output? A) The output becomes a 1D vector B) The output maintains the same dimensions as the input image C) The output becomes a 3D tensor with depth equal to the number of filters D) The output reduces in size based on the number of filters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C. The output becomes a 3D tensor with depth equal to the number of filters.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question (or 'exit'):  When computing gradients for a neural network with a convolutional layer followed by ReLU activation, which component ensures that updates only occur for activated neurons?  A) The partial derivative of ZÂ¹ with respect to WÂ¹ B) The diagonal matrix where elements are 1 for positive inputs and 0 otherwise C) The transpose of the weight matrix WÂ² D) The squared loss function derivative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Response:\n",
      "==================================================\n",
      "C) The diagonal matrix where elements are 1 for positive inputs and 0 otherwise (also known as the ReLU derivative or the ReLU activation function's derivative)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mQuestion (or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def load_model():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    prompt = prompt = f\"\"\"<s>[INST] Just give the correct answer. No explanation is required.\n",
    "\n",
    "    Question: {question} [/INST]\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=40,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model()\n",
    "    while True:\n",
    "        question = input(\"\\nQuestion (or 'exit'): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        try:\n",
    "            response = generate_response(model, tokenizer, question)\n",
    "            print(\"\\nBase Model Response:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
